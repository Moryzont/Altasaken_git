q = quoted_term,  # Use the quoted term directly
searchType = "FULL_TEXT_SEARCH",
digitalAccessibleOnly = "false",
expand = "metadata",
fragments = "2",
fragSize = "500",
profile = "wwwnbno",
page = as.character(page_number),
size = "100"
)
# Handle 'filter' parameters separately
filters_list <- setNames(as.list(filter_values), rep("filter", length(filter_values)))
# Combine the parameters
query_params <- c(params, filters_list)
# For debugging: print the full URL being requested
# full_url <- modify_url(base_url, query = query_params)
# cat("Requesting URL:", full_url, "\n")
# Make the GET request
response <- GET(
url = base_url,
query = query_params,
add_headers(`accept` = "application/hal+json")
)
# Check the HTTP status code
if (http_status(response)$category != "Success") {
warning("API request failed for term '", term, "' on page ", page_number, " with status: ", http_status(response)$message)
break  # Exit the pagination loop and proceed to the next term
}
# Parse the JSON content
content_text <- content(response, as = "text", encoding = "UTF-8")
json_data <- fromJSON(content_text, flatten = TRUE)
# Update total_pages after the first request
if (page_number == 0) {
total_pages <- json_data$page$totalPages
cat("Total pages for term '", term, "': ", total_pages, "\n", sep = "")
}
# Check if there are items in the response
if (!is.null(json_data$`_embedded`$items)) {
# Access the items in the response
items <- json_data$`_embedded`$items
# Check if 'metadata.identifiers.urn' exists in items
if ("metadata.identifiers.urn" %in% names(items)) {
# Extract the IDs
ids <- items$`metadata.identifiers.urn`
# Append the IDs to the all_ids vector
all_ids <- c(all_ids, ids)
cat("Collected ", length(ids), " IDs from page ", page_number + 1, " of ", total_pages, "\n", sep = "")
} else {
# Handle the case where the column doesn't exist
cat("No URNs found for term '", term, "' on page ", page_number, "\n", sep = "")
}
} else {
# No items found for this page
cat("No results found for term '", term, "' on page ", page_number, "\n", sep = "")
}
# Increment the page number
page_number <- page_number + 1
# Pause between requests to be polite to the server
Sys.sleep(0.001)
}
}
# Remove duplicate IDs
unique_ids <- unique(all_ids)
# Print the total number of unique IDs found
cat("Total unique IDs found for year ", year, ": ", length(unique_ids), "\n", sep = "")
# Ensure unique_ids is not empty
if (length(unique_ids) == 0) {
warning("No URNs found for year ", year, ". Skipping to next year.")
next
}
# Define the get_content_fragments function
get_content_fragments <- function(id, query, fragments = 3, frag_size = 300) {
# Add double quotes around the term for exact phrase search
quoted_query <- paste0('"', query, '"')
# Construct the URL
url <- paste0("https://api.nb.no/catalog/v1/items/", id, "/contentfragments")
# Construct the query parameters
query_params <- list(
q = quoted_query,
fragments = fragments,
fragSize = frag_size
)
# Send GET request
response <- GET(
url,
query = query_params,
add_headers("accept" = "application/hal+json")
)
# Check if the request was successful
if (status_code(response) == 200) {
content_json <- content(response, "text", encoding = "UTF-8")
content <- suppressMessages(fromJSON(content_json, flatten = TRUE))
# If contentFragments are empty, return NULL
if (length(content$contentFragments) == 0) {
return(NULL)
}
return(content$contentFragments)
} else {
# Handle request failure
return(NULL)
}
}
# Initialize counters
total_urns_with_fragments <- 0
total_fragments <- 0
# Initialize a list to store the results
all_results <- list()
# Initialize progress bar
pb <- progress_bar$new(
format = "  Processing URNs [:bar] :percent | :current/:total | Elapsed: :elapsed",
total = length(unique_ids), clear = FALSE, width = 60
)
# Loop over each URN
for (id in unique_ids) {
# Initialize a list to store results for this URN
urn_results <- list()
# Initialize fragment count for this URN
urn_fragment_count <- 0
# Loop over each search term
for (term in search_terms) {
# Fetch content fragments
fragments <- get_content_fragments(id, term)
if (!is.null(fragments)) {
# Store the fragments in the urn_results list under the search term
urn_results[[term]] <- fragments
# Update fragment counts
urn_fragment_count <- urn_fragment_count + length(fragments)
total_fragments <- total_fragments + length(fragments)
}
# Pause between requests to be polite to the server
Sys.sleep(0.001)
}
# Only add URN to results if there are any fragments
if (length(urn_results) > 0) {
# Store the urn_results in the all_results list under the URN
all_results[[id]] <- urn_results
# Update URN count
total_urns_with_fragments <- total_urns_with_fragments + 1
}
# Update progress bar
pb$tick()
}
# Print total counts
cat("\nTotal URNs with fragments for year ", year, ": ", total_urns_with_fragments, "\n", sep = "")
cat("Total content fragments collected for year ", year, ": ", total_fragments, "\n", sep = "")
# Load necessary library
if (!require(dplyr)) install.packages("dplyr")
library(dplyr)
# Initialize an empty data frame
result_df <- data.frame(URN = character(), Text = character(), stringsAsFactors = FALSE)
# Iterate over each URN in all_results
for (urn in names(all_results)) {
# Get the list of fragments for this URN
urn_results <- all_results[[urn]]
# Initialize a vector to hold all fragments for this URN
all_fragments <- c()
# Loop over each search term
for (term in names(urn_results)) {
# Get the fragments for this term
fragments <- urn_results[[term]]
# Check if fragments are not empty
if (!is.null(fragments) && length(fragments) > 0) {
# Depending on the structure of fragments, extract the 'text' field
if (is.data.frame(fragments) && "text" %in% names(fragments)) {
fragment_texts <- fragments$text
} else if (is.list(fragments)) {
# If fragments is a list, extract 'text' from each element
fragment_texts <- sapply(fragments, function(x) x$text)
} else {
# If the structure is different, print a warning
warning(paste("Unexpected structure in fragments for URN:", urn, "and term:", term))
next
}
# Collect all fragments
all_fragments <- c(all_fragments, fragment_texts)
}
}
# Remove duplicate fragments for this URN
unique_fragments <- unique(all_fragments)
# Remove '<em>' tags from the text using regex
cleaned_fragments <- gsub("<[^>]+>", "", unique_fragments)
# Create a data frame with URN and cleaned text
urn_df <- data.frame(URN = rep(urn, length(cleaned_fragments)), Text = cleaned_fragments, stringsAsFactors = FALSE)
# Append to the result_df
result_df <- bind_rows(result_df, urn_df)
}
# Save the results to a CSV file
write.csv(result_df, paste0(year, "_conc_urn.csv"), row.names = FALSE)
# Read the CSV files
result_df <- read.csv(paste0(year, "_conc_urn.csv"))
# Read the labeled sentences CSV file (ensure it exists for each year)
labeled_file <- paste0("setninger_labeled_MNLI_", year, ".csv")
if (!file.exists(labeled_file)) {
warning("Labeled file ", labeled_file, " does not exist. Skipping merging step for year ", year, ".")
next
}
combined_df <- read.csv(labeled_file, stringsAsFactors = FALSE)
##### Combining and adding metadata
# Rename columns for consistency
names(result_df)[names(result_df)=="Text"] <- "sentence"
# Add URN
combined_URN <- merge(result_df, combined_df, by = "sentence", all.x = TRUE)
if (!require(tidyr)) install.packages("tidyr")
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyr)
library(tidyverse)
# Split the URN into multiple columns
combined_URN <- combined_URN %>%
separate(URN, into = paste0("URN_part", 1:10), sep = "_", fill = "right", remove = FALSE)
# Extract the desired components
combined_URN <- combined_URN %>%
mutate(
title = URN_part3,
timestamp = URN_part6,
year = substr(timestamp, 1, 4)
)
# Remove the temporary URN_part columns
combined_URN <- combined_URN %>%
select(-starts_with("URN_part"))
# Remove duplicates
combined_URN <- combined_URN %>%
distinct(.keep_all = TRUE)
# Add empty columns for city, dhlabid, and date
combined_URN[ , 'city'] <- NA
combined_URN[ , 'dhlabid'] <- NA
combined_URN[ , 'date'] <- NA
# Rename columns
names(combined_URN)[names(combined_URN)=="URN"] <- "urn"
names(combined_URN)[names(combined_URN)=="sentence"] <- "conc"
# Add city based on title (you can customize this mapping as needed)
city_mapping <- list(
"agder" = "Flekkefjord",
"dittoslogrunerloekkasagene" = "Oslo",
# Add other mappings here...
"lofottidende" = "Leknes"
)
combined_URN <- combined_URN %>%
mutate(city = city_mapping[[title]])
# Sort columns
combined_URN_sortert = combined_URN[,c("city", "conc", "timestamp", "title", "year", "urn", "dhlabid", "date", setdiff(names(combined_URN), c("city", "conc", "timestamp", "title", "year", "urn", "dhlabid", "date")))]
# Convert timestamp to date
if (!require(lubridate)) install.packages("lubridate")
library(lubridate)
combined_URN_sortert <- combined_URN_sortert %>%
mutate(
date = ymd(timestamp)
)
# Save the cleaned data to a CSV file
write.csv(combined_URN_sortert, paste0(year, "_conc_renset.csv"), row.names = FALSE)
cat("Completed processing for year:", year, "\n")
}
# Clear the "Global Environment"
rm(list = ls())
# Adjust the path as needed
setwd("/Users/vetlewisloffsandring/Documents/Altasaken")
options(scipen = 999)
# Clear the "Global Environment"
rm(list = ls())
# Adjust the path as needed
setwd("/Users/vetlewisloffsandring/Documents/Altasaken/Testing")
options(scipen = 999)
# Install and load required packages
required_packages <- c("httr", "jsonlite", "progress", "data.table", "foreach", "doParallel", "lubridate", "tidyr", "dplyr")
for (pkg in required_packages) {
if (!require(pkg, character.only = TRUE)) install.packages(pkg)
}
library(httr)
library(jsonlite)
library(progress)
library(data.table)
library(foreach)
library(doParallel)
library(lubridate)
library(tidyr)
library(dplyr)
# Base URL of the API
base_url <- "https://api.nb.no/catalog/v1/items"
# Define your search terms as a character vector
search_terms <- c(
"Alta saken", "Alta-saken", "Alta- saken", "Altasaken",
"Alta saka", "Alta-saka", "Alta- saka", "Altasaka",
"Alta konflikten", "Alta-konflikten", "Alta- konflikten", "Altakonflikten",
"konflikten i Alta",
"Alta utbyggingen", "Alta-utbyggingen", "Alta- utbyggingen", "Altautbyggingen",
"Alta ut- byggingen", "Alta-ut- byggingen", "Altaut- byggingen",
"Alta utbygginga", "Alta-utbygginga", "Alta- utbygginga", "Altautbygginga",
"utbyggingen av Alta", "utbyggingen av Alta-", "utbyggingen av Altaelva", "utbyggingen av Altaelven",
"utbygginga av Alta", "utbygginga av Alta-", "utbygginga av Altaelva", "utbygginga av Altaelven",
"Alta demningen", "Alta-demningen", "Alta- demingen", "Altademningen",
"Alta demninga", "Alta-demninga", "Alta- demninga", "Altademninga",
"Alta dammen", "Alta-dammen", "Alta- dammen", "Altadammen",
"Folkeaksjonen mot utbyggingen", "Folke-aksjonen mot utbyggingen",
"Folkeaksjonen mot utbygginga", "Folke-aksjonen mot utbygginga",
"demonstrantane i Stilla", "demonstrantene i Stilla", "demonstranter i Stilla", "demonstrerte i Stilla",
"Alta demonstrantene", "Alta-demonstrantene", "Alta- demonstrantene", "Altademonstrantene",
"Alta aksjonen", "Alta-aksjonen", "Alta- aksjonen", "Altaaksjonen",
"Aksjonen i Alta", "Aksjonistene i Alta", "Aksjonen i Stilla",
"Alta-Kautokeinovassdraget", "Alta- Kautokeinovassdraget", "Alta-Kautokeino vassdraget", "Alta- Kautokeino- vassdraget", "Alta-Kautokeino- vassdraget",
"Alta-vassdraget", "Alta- Vassdraget", "Alta vassdraget",
"Alta-kautokeino utbyggingen", "Alta- kautokeino utbyggingen",
"Slaget i Stilla"
)
# Prepare for parallel processing
num_cores <- detectCores() - 1  # Reserve one core for the system
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Loop over each year from 1979 to 2022
# Loop over each year from 1979 to 2022
for (year in 1985:1988) {
cat("\nProcessing year:", year, "\n")
# Initialize per-year variables
all_ids <- list()  # Use a list to associate URNs with search terms
# Adjust filter_values for the current year
filter_values <- c(paste0("year:", year), "mediatype:aviser")
# Initialize progress bar for search terms
pb_terms <- txtProgressBar(min = 0, max = length(search_terms), style = 3)
term_index <- 0
# Loop over each search term
for (term in search_terms) {
term_index <- term_index + 1
setTxtProgressBar(pb_terms, term_index)
# Add double quotes around the term for exact phrase search
quoted_term <- paste0('"', term, '"')
# Initialize pagination variables
page_number <- 0  # Zero-based index
total_pages <- 1  # Default value, will be updated after first request
# Loop over pages
repeat {
# Query parameters without the 'filter' parameter
params <- list(
q = quoted_term,  # Use the quoted term directly
searchType = "FULL_TEXT_SEARCH",
digitalAccessibleOnly = "false",
expand = "metadata",
fragments = "2",
fragSize = "500",
profile = "wwwnbno",
page = as.character(page_number),
size = "100"
)
# Handle 'filter' parameters separately
filters_list <- setNames(as.list(filter_values), rep("filter", length(filter_values)))
# Combine the parameters
query_params <- c(params, filters_list)
# Make the GET request
response <- GET(
url = base_url,
query = query_params,
add_headers(`accept` = "application/hal+json")
)
# Check the HTTP status code
if (http_status(response)$category != "Success") {
warning("API request failed for term '", term, "' on page ", page_number, " with status: ", http_status(response)$message)
break  # Exit the pagination loop and proceed to the next term
}
# Parse the JSON content
content_text <- content(response, as = "text", encoding = "UTF-8")
json_data <- fromJSON(content_text, flatten = TRUE)
# Update total_pages after the first request
if (page_number == 0) {
total_pages <- json_data$page$totalPages
}
# Check if there are items in the response
if (!is.null(json_data$`_embedded`$items)) {
# Access the items in the response
items <- json_data$`_embedded`$items
# Check if 'metadata.identifiers.urn' exists in items
if ("metadata.identifiers.urn" %in% names(items)) {
# Extract the IDs
ids <- items$`metadata.identifiers.urn`
# Associate each URN with the search term
for (id in ids) {
if (id %in% names(all_ids)) {
all_ids[[id]] <- unique(c(all_ids[[id]], term))
} else {
all_ids[[id]] <- term
}
}
}
}
# Break the loop if we've reached the last page
page_number <- page_number + 1
if (page_number >= total_pages) break
# Pause between requests to be polite to the server
Sys.sleep(0.01)
}
}
close(pb_terms)
# Get unique URNs
unique_ids <- names(all_ids)
# Print the total number of unique IDs found
cat("\nTotal unique IDs found for year ", year, ": ", length(unique_ids), "\n", sep = "")
# Ensure unique_ids is not empty
if (length(unique_ids) == 0) {
warning("No URNs found for year ", year, ". Skipping to next year.")
next
}
# Define the get_content_fragments function
get_content_fragments <- function(id, terms, fragments = 3, frag_size = 300) {
# Initialize a list to store fragments per term
fragments_list <- list()
for (term in terms) {
# Add double quotes around the term for exact phrase search
quoted_query <- paste0('"', term, '"')
# Construct the URL
url <- paste0("https://api.nb.no/catalog/v1/items/", id, "/contentfragments")
# Construct the query parameters
query_params <- list(
q = quoted_query,
fragments = fragments,
fragSize = frag_size
)
# Send GET request
response <- GET(
url,
query = query_params,
add_headers("accept" = "application/hal+json")
)
# Check if the request was successful
if (status_code(response) == 200) {
content_json <- content(response, "text", encoding = "UTF-8")
content <- suppressMessages(fromJSON(content_json, flatten = TRUE))
# If contentFragments are not empty, store them
if (!is.null(content$contentFragments) && length(content$contentFragments) > 0) {
fragments_list[[term]] <- content$contentFragments
}
}
# Pause between requests to be polite to the server
Sys.sleep(0.001)
}
return(fragments_list)
}
# Initialize counters
total_urns_with_fragments <- 0
total_fragments <- 0
# Initialize a list to store the results
all_results <- list()
# Initialize progress bar for URNs
pb_urns <- txtProgressBar(min = 0, max = length(unique_ids), style = 3)
urn_index <- 0
# Use foreach for parallel processing of URNs
fragments_list <- foreach(id = unique_ids, .packages = c("httr", "jsonlite")) %dopar% {
terms <- all_ids[[id]]
get_content_fragments(id, terms)
}
# Combine results
names(fragments_list) <- unique_ids
# Process the collected fragments
for (i in seq_along(unique_ids)) {
id <- unique_ids[i]
urn_results <- fragments_list[[i]]
# Update progress bar
urn_index <- urn_index + 1
setTxtProgressBar(pb_urns, urn_index)
# Check if any fragments were found
if (length(urn_results) > 0) {
all_results[[id]] <- urn_results
total_urns_with_fragments <- total_urns_with_fragments + 1
# Update total_fragments count
for (term_fragments in urn_results) {
total_fragments <- total_fragments + length(term_fragments)
}
}
}
close(pb_urns)
# Print total counts
cat("\nTotal URNs with fragments for year ", year, ": ", total_urns_with_fragments, "\n", sep = "")
cat("Total content fragments collected for year ", year, ": ", total_fragments, "\n", sep = "")
# Initialize an empty data table
result_dt <- data.table(URN = character(), Text = character())
# Iterate over each URN in all_results
for (urn in names(all_results)) {
# Get the list of fragments for this URN
urn_results <- all_results[[urn]]
# Initialize a vector to hold all fragments for this URN
all_fragments <- c()
# Loop over each search term
for (term in names(urn_results)) {
# Get the fragments for this term
fragments <- urn_results[[term]]
# Check if fragments are not empty
if (!is.null(fragments) && length(fragments) > 0) {
# Depending on the structure of fragments, extract the 'text' field
if (is.data.frame(fragments) && "text" %in% names(fragments)) {
fragment_texts <- fragments$text
} else if (is.list(fragments)) {
# If fragments is a list, extract 'text' from each element
fragment_texts <- sapply(fragments, function(x) x$text)
} else {
# If the structure is different, print a warning
warning(paste("Unexpected structure in fragments for URN:", urn, "and term:", term))
next
}
# Collect all fragments
all_fragments <- c(all_fragments, fragment_texts)
}
}
# Remove duplicate fragments for this URN
unique_fragments <- unique(all_fragments)
# Remove '<em>' tags from the text using regex
cleaned_fragments <- gsub("<[^>]+>", "", unique_fragments)
# Create a data table with URN and cleaned text
urn_dt <- data.table(URN = urn, Text = cleaned_fragments)
# Append to the result_dt
result_dt <- rbind(result_dt, urn_dt)
}
# Clear the "Global Environment"
rm(list = ls())
# Clear the "Global Environment"
rm(list = ls())
# Clear the "Global Environment"
rm(list = ls())
